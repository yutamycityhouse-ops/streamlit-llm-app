from dotenv import load_dotenv
load_dotenv()

import streamlit as st
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.messages import SystemMessage, HumanMessage  â† ä¿®æ­£ç‚¹1: ã“ã®è¡Œã‚’å‰Šé™¤ã¾ãŸã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆ
from langchain_core.output_parsers import StrOutputParser

# --- é–¢æ•°ã®å®šç¾© ---

# ã€æ¡ä»¶ã€‘ã€Œå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã€ã¨ã€Œãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ã®é¸æŠå€¤ã€ã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å–ã‚Šã€
# LLMã‹ã‚‰ã®å›ç­”ã‚’æˆ»ã‚Šå€¤ã¨ã—ã¦è¿”ã™é–¢æ•°
def get_llm_response(user_input, expertise_choice):
    """
    LLMã‹ã‚‰ã®å›ç­”ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    
    # ã€æ¡ä»¶ã€‘ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ã§ã®é¸æŠå€¤ã«å¿œã˜ã¦LLMã«æ¸¡ã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å¤‰ãˆã‚‹
    # å°‚é–€å®¶ã®ç¨®é¡ã¯ã€Œå¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã€ã¨ã€Œã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã€ã¨ã—ã¾ã™ã€‚
    if expertise_choice == "å¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼":
        system_message_content = "ã‚ãªãŸã¯å„ªç§€ãªå¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€å¥åº·çš„ã§å®Ÿè·µçš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ç°¡æ½”ã«æä¾›ã—ã¦ãã ã•ã„ã€‚"
    elif expertise_choice == "ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ":
        system_message_content = "ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚­ãƒ£ãƒªã‚¢ã«é–¢ã™ã‚‹æ‚©ã¿ã‚„è³ªå•ã«å¯¾ã—ã€å…·ä½“çš„ã§å‰å‘ããªåŠ©è¨€ã‚’ç°¡æ½”ã«è¡Œã£ã¦ãã ã•ã„ã€‚"
    else:
        system_message_content = "ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"

    # ã€æ¡ä»¶ã€‘Lesson8ã‚’å‚è€ƒã«LangChainã®ã‚³ãƒ¼ãƒ‰ã‚’è¨˜è¿°
    
    # 1. LLMãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7) 

    # 2. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®ä½œæˆ (â† ä¿®æ­£ç‚¹2: ã‚¿ãƒ—ãƒ«è¨˜æ³•ã«å¤‰æ›´)
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message_content),
        ("human", "{user_question}")
    ])

    # 3. å‡ºåŠ›ãƒ‘ãƒ¼ã‚µãƒ¼
    output_parser = StrOutputParser()

    # 4. ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
    chain = prompt | llm | output_parser
    
    # 5. LLMã®å®Ÿè¡Œã¨å›ç­”ã®å–å¾—
    try:
        response = chain.invoke({"user_question": user_input})
        return response
    except Exception as e:
        # APIã‚­ãƒ¼ãŒãªã„å ´åˆã‚„ä»–ã®ã‚¨ãƒ©ãƒ¼ã‚’ã‚­ãƒ£ãƒƒãƒ
        st.error(f"AIã®å‘¼ã³å‡ºã—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        st.error("ã€é–‹ç™ºè€…å‘ã‘ã€‘: .envãƒ•ã‚¡ã‚¤ãƒ«ã«OPENAI_API_KEYãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

# --- Streamlit ã‚¢ãƒ—ãƒªã®UI ---

st.title("ğŸ§‘â€ğŸ« å°‚é–€å®¶ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
st.markdown("""
ã“ã®ã‚¢ãƒ—ãƒªã¯ã€ã‚ãªãŸã®è³ªå•ã‚„æ‚©ã¿ã«å°‚é–€å®¶ãŒå›ç­”ã™ã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã§ã™ã€‚
""")

st.info("**ã€æ“ä½œæ–¹æ³•ã€‘**\n1. ç›¸è«‡ã—ãŸã„å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚\n2. è³ªå•ã‚’å…¥åŠ›ã—ã€ã€Œé€ä¿¡ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")

# --- UIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ ---

expertise = st.radio(
    "ç›¸è«‡ã™ã‚‹å°‚é–€å®¶ã‚’é¸ã‚“ã§ãã ã•ã„:",
    ("å¥åº·ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼", "ã‚­ãƒ£ãƒªã‚¢ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆ"),
    key="expertise_choice"
)

user_query = st.text_area("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", "", height=150)

if st.button("é€ä¿¡"):
    if user_query:
        st.info(f"**ã‚ãªãŸã®è³ªå•:**\n{user_query}")
        st.info(f"**é¸æŠã—ãŸå°‚é–€å®¶:** {expertise}")
        
        with st.spinner("AIãŒå›ç­”ã‚’ç”Ÿæˆä¸­ã§ã™..."):
            answer = get_llm_response(user_query, expertise)
            
            if answer:
                st.success(f"**{expertise}ã‹ã‚‰ã®å›ç­”:**\n{answer}")
    else:
        st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")